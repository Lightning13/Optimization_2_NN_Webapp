is_package: true
container:
  type: HtmlTemplate
  properties: {html: '@theme:standard-page.html'}
components:
- type: ColumnPanel
  properties: {}
  name: content_panel
  layout_properties: {slot: default}
  components:
  - name: rich_text_1
    properties: {content: '**Our CNN model**'}
    type: RichText
    layout_properties: {grid_position: 'SFHDEN,JTXSYL'}
  - name: rich_text_2
    properties: {content: '**CNN**


        In our journey to harness the full potential of Convolutional Neural Networks
        (**CNNs**) for the **MNIST dataset**, we embarked on an exploration that delved
        deep into the intricacies of network design using both **TensorFlow** and
        **PyTorch**. Our aim was not just to adhere to the **TensorFlow** framework
        as suggested by our course guidelines but to extend our investigation into
        **PyTorch**, motivated by a curiosity to discern any impactful differences
        between the two frameworks.


        **TensorFlow Model**


        In our **TensorFlow** model designed for the **MNIST dataset**, the architecture
        was meticulously crafted to optimize performance through a blend of convolutional
        layers, batch normalization, ReLU activations, and strategic dropout implementation.
        This model represents a harmonious integration of complexity and regularization,
        tailored to capture the intricate patterns of handwritten digits with high
        accuracy.


        **Initial Architecture and Adjustments**


        Our model initiates with a first convolutional block consisting of two convolutional
        layers, each equipped with 32 filters of size 3x3. This choice was guided
        by the aim to commence feature extraction effectively, identifying basic shapes
        and edges crucial for early digit recognition. After observing initial results,
        it became apparent that while this setup was effective for basic patterns,
        enhancing the model''s depth was necessary for capturing more complex features.


        Moving to the second convolutional block, we increased the filter count to
        64 while maintaining the kernel size. This adjustment was based on our experimentation,
        which showed that a higher number of filters at this stage allowed the network
        to build a more nuanced understanding of the input images, such as the curves
        and intersections unique to different digits.


        **Key Breakthrough with Dropout**


        A significant turning point in our model''s development was our approach to
        dropout regularization. Initially, we implemented a conservative dropout rate
        of 0.3 after the max-pooling layers in our first and second convolutional
        blocks. This was based on early experiments indicating minor overfitting,
        where we observed a slight but noticeable gap between training and validation
        accuracies, approximately 98.5% to 98% respectively.


        However, the real breakthrough came when we decided to further refine the
        dropout strategy. Specifically, after the third convolutional block, where
        we had escalated the filter count to 128 to delve into even more detailed
        feature extraction, we increased the dropout rate to 0.4. This was a calculated
        decision to counteract the increased complexity and potential overfitting
        associated with deeper layers.


        The final dense layer before classification was another critical focus area.
        Recognizing the importance of this layer in interpreting the high-level features
        extracted by the convolutional blocks, we introduced a dropout rate of 0.5.
        This higher rate before the softmax classification layer marked a pivotal
        adjustment, ensuring that our model maintained robustness and generalization
        ability, effectively narrowing the gap between training and validation accuracy
        to a mere 0.2%.


        **Testing and Results Analysis**


        **PyTorch Model**


        For our **PyTorch** model designed to classify the **MNIST dataset**, we meticulously
        crafted an architecture that encapsulates our understanding of effective deep
        learning practices. The model is a testament to our iterative design process,
        where each layer and technique is purposefully selected to enhance the model''s
        ability to discern and classify complex patterns in handwritten digits.


        **Architecture and Strategic Enhancements**


        Our **PyTorch** model, **CNN**, begins its feature extraction journey with
        a convolutional layer sequence designed for progressive complexity. The initial
        layer starts with 32 filters of size 3x3, a decision made to capture the fundamental
        aspects of the images such as edges and basic shapes. This is immediately
        followed by batch normalization and ReLU activation to ensure a stable and
        non-linear processing of the input data. The inclusion of a max-pooling layer
        after each convolutional and activation sequence serves to reduce the spatial
        dimensions while preserving the most salient features.


        As we advance deeper into the network, the number of filters in the convolutional
        layers increases from 32 to 64, and finally to 128. This gradual increase
        is not arbitrary; it is based on our empirical findings that a deeper network
        with more filters at later stages is more adept at capturing the intricate
        details necessary for accurate digit classification. The consistent application
        of batch normalization across these layers plays a crucial role in maintaining
        the efficiency of the training process by normalizing the layer inputs.


        **Critical Role of Dropout in Regularization**


        A pivotal aspect of our **PyTorch** model is the strategic use of dropout,
        particularly within the fully connected layers. Recognizing the potential
        for overfitting with increased model complexity, we implemented a dropout
        rate of 0.5 both before and after the dense layer comprising 512 units. This
        aggressive regularization approach significantly contributed to the model''s
        robustness, ensuring that it generalizes well to unseen data by preventing
        it from relying too heavily on any single feature.


        **Forward Pass and Model Integration**


        The forward pass of our model is streamlined yet powerful, with the convolutional
        layers feeding into a flattened layer that transforms the 2D feature maps
        into a 1D vector. This vector is then processed through the fully connected
        layers, where the dropout regularization is applied. The culmination of this
        process is a softmax output that classifies the input image into one of the
        ten digit categories.


        **Testing and Results Analysis**


        Our PyTorch model also demonstrated impressive performance on the MNIST dataset,
        with only a few instances of misclassification. Specifically, the model misclassified
        the digit ''1'' as ''2'', ''6'' as ''8'', and ''7'' as ''3''. While the overall
        accuracy remains high, these misclassifications provide valuable insights
        into areas where the model could be further refined'}
    type: RichText
    layout_properties: {grid_position: 'DLIKWH,ZNYANY'}
  - name: label_1
    properties: {text: 1 MISCLASSIFIED AS 2, align: center, bold: true}
    type: Label
    layout_properties: {grid_position: 'PVBMVA,FBWBHX'}
  - name: label_2
    properties: {text: 6 MISCLASSIFIED AS 8, align: center, bold: true}
    type: Label
    layout_properties: {grid_position: 'PVBMVA,VXYPYR'}
  - name: label_3
    properties: {text: 7 MISCLASSIFIED AS 3, bold: true, align: center}
    type: Label
    layout_properties: {grid_position: 'PVBMVA,HUIXBH'}
  - name: image_1
    properties: {source: _/theme/1_missclassified.png, height: 257.75, spacing_above: none,
      spacing_below: none}
    type: Image
    layout_properties: {grid_position: 'DGFUNY,MQNDYS'}
  - name: image_2
    properties: {source: _/theme/6_misclassified.png, height: 256.75, spacing_above: none,
      spacing_below: none}
    type: Image
    layout_properties: {grid_position: 'DGFUNY,UFILDN'}
  - name: image_3
    properties: {source: _/theme/7_misclassified.png, height: 255.75, spacing_above: none,
      spacing_below: none}
    type: Image
    layout_properties: {grid_position: 'DGFUNY,PNDOFH'}
  - name: rich_text_3
    properties: {content: 'The misclassification of ''1'' as ''2'' and ''7'' as ''3''
        might be attributed to the similarity in certain handwritten forms of these
        digits. For instance, a ''1'' with a longer top stroke could resemble a ''2'',
        and a ''7'' with a curved line might be mistaken for a ''3''. The confusion
        between ''6'' and ''8'' could arise from the closed loop in their structures,
        where a slightly distorted ''6'' might appear similar to an ''8''.


        To address these issues, further fine-tuning of the model could be considered.
        For example, increasing the complexity of the convolutional layers or adjusting
        the dropout rates might help the model better distinguish between similar-looking
        digits. Additionally, augmenting the training dataset with more varied examples
        of these challenging digits could improve the model''s ability to recognize
        them correctly.


        In conclusion, both our TensorFlow and PyTorch models have shown excellent
        performance in classifying the MNIST dataset. While the TensorFlow model achieved
        perfect accuracy, the PyTorch model''s few misclassifications offer opportunities
        for further refinement. These results underscore the effectiveness of our
        CNN architectures and the importance of continual optimization to enhance
        model performance.

        '}
    type: RichText
    layout_properties: {grid_position: 'NVMAMB,EPVCWX'}
- type: FlowPanel
  properties: {}
  name: navbar_links
  layout_properties: {slot: nav-right}
- name: button_1
  properties: {text: Go Back, icon: 'fa:arrow-left', background: 'theme:Secondary
      Container'}
  type: Button
  layout_properties: {slot: title}
  event_bindings: {click: button_1_click}
- name: button_2
  properties: {text: 'Dive into our models?', bold: true, icon: 'fa:arrow-circle-right',
    font_size: 29}
  type: Button
  layout_properties: {slot: default}
  event_bindings: {click: button_2_click}
